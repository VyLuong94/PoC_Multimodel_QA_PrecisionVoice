{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéôÔ∏è PrecisionVoice - Vietnamese Speech-to-Text\n",
        "\n",
        "Notebook ƒë∆°n gi·∫£n ƒë·ªÉ transcribe audio ti·∫øng Vi·ªát s·ª≠ d·ª•ng **faster-whisper** v√† **pyannote** (diarization).\n",
        "\n",
        "### H∆∞·ªõng d·∫´n\n",
        "1. **Ch·ªçn GPU**: `Runtime` ‚Üí `Change runtime type` ‚Üí **T4 GPU**\n",
        "2. **C√†i ƒë·∫∑t Secrets**: Th√™m `HF_TOKEN` v√†o Colab Secrets (Key icon b√™n tr√°i) ƒë·ªÉ d√πng Pyannote.\n",
        "3. **Ch·∫°y t·ª´ng cell** theo th·ª© t·ª± t·ª´ tr√™n xu·ªëng\n",
        "4. **S·ª≠ d·ª•ng Gradio link** ·ªü cell cu·ªëi ƒë·ªÉ truy c·∫≠p UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 1. üîç Ki·ªÉm tra GPU\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if device == \"cuda\":\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"‚úÖ GPU Detected: {gpu_name}\")\n",
        "    print(f\"   VRAM: {gpu_mem:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è KH√îNG T√åM TH·∫§Y GPU!\")\n",
        "    print(\"üëâ V√†o Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 2. üì¶ C√†i ƒë·∫∑t Dependencies\n",
        "print(\"Installing dependencies...\")\n",
        "!pip install --upgrade torch torchvision torchaudio \"pyannote.audio>=3.3.1\" faster-whisper gradio librosa nest_asyncio lightning torchmetrics\n",
        "!apt-get install -y -qq ffmpeg > /dev/null 2>&1\n",
        "print(\"‚úÖ Dependencies installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 3. ü§ñ Load Models (Whisper & Pyannote)\n",
        "import torch\n",
        "import time\n",
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "from google.colab import userdata\n",
        "from faster_whisper import WhisperModel\n",
        "from pyannote.audio import Pipeline\n",
        "\n",
        "try:\n",
        "    from pyannote.audio.core.task import Specifications, Problem, Resolution\n",
        "    torch.serialization.add_safe_globals([Specifications, Problem, Resolution])\n",
        "except Exception as e:\n",
        "    print(f\"Could not add custom globals: {e}\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "compute_type = \"float16\" if device == \"cuda\" else \"int8\"\n",
        "\n",
        "# Danh s√°ch c√°c model Whisper h·ªó tr·ª£\n",
        "AVAILABLE_MODELS = {\n",
        "    \"EraX-WoW-Turbo (Whisper Large V3 Turbo - Ti·∫øng Vi·ªát)\": \"erax-ai/EraX-WoW-Turbo-V1.1-CT2\",\n",
        "    \"PhoWhisper Large (Ti·∫øng Vi·ªát)\": \"kiendt/PhoWhisper-large-ct2\"\n",
        "}\n",
        "\n",
        "# Cache models\n",
        "loaded_whisper_models = {}\n",
        "diarization_pipeline = None\n",
        "\n",
        "# L·∫•y HF_TOKEN\n",
        "try:\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "except:\n",
        "    hf_token = os.environ.get('HF_TOKEN')\n",
        "\n",
        "# ==================== LOAD ALL WHISPER MODELS ====================\n",
        "print(\"=\"*50)\n",
        "print(\"üîÑ Pre-downloading ALL Whisper Models...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "total_start = time.time()\n",
        "for model_name, model_path in AVAILABLE_MODELS.items():\n",
        "    print(f\"\\nüì• Loading: {model_name}\")\n",
        "    start = time.time()\n",
        "    try:\n",
        "        model = WhisperModel(\n",
        "            model_path,\n",
        "            device=device,\n",
        "            compute_type=compute_type\n",
        "        )\n",
        "        loaded_whisper_models[f\"{model_name}_{compute_type}\"] = model\n",
        "        print(f\"   ‚úÖ Loaded in {time.time() - start:.1f}s\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Failed to load: {e}\")\n",
        "\n",
        "print(f\"\\n‚úÖ All models loaded in {time.time() - total_start:.1f}s\")\n",
        "print(f\"   Total models: {len(loaded_whisper_models)}\")\n",
        "print(f\"   Device: {device}, Compute: {compute_type}\")\n",
        "\n",
        "# ==================== LOAD PYANNOTE ====================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üîÑ Loading Pyannote Diarization...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "if not hf_token:\n",
        "    print(\"‚ö†Ô∏è WARNING: HF_TOKEN not found!\")\n",
        "    print(\"   Diarization will be disabled.\")\n",
        "    print(\"   Please set HF_TOKEN in Colab Secrets.\")\n",
        "else:\n",
        "    start = time.time()\n",
        "    try:\n",
        "        diarization_pipeline = Pipeline.from_pretrained(\n",
        "            \"pyannote/speaker-diarization-community-1\",\n",
        "            token=hf_token\n",
        "        )\n",
        "        diarization_pipeline.to(torch.device(device))\n",
        "        print(f\"‚úÖ Pyannote loaded in {time.time() - start:.1f}s\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to load Pyannote: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üéâ All models loaded successfully!\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 4. üõ†Ô∏è Utilities & Helpers\n",
        "import gradio as gr\n",
        "import time\n",
        "import nest_asyncio\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "def convert_audio_to_wav(audio_path):\n",
        "    \"\"\"Chu·∫©n h√≥a audio v·ªÅ ƒë·ªãnh d·∫°ng WAV 16kHz Mono.\"\"\"\n",
        "    try:\n",
        "        # T·∫°o file t·∫°m\n",
        "        output_path = \"temp_processed_audio.wav\"\n",
        "        \n",
        "        # X√≥a file c≈© n·∫øu t·ªìn t·∫°i\n",
        "        if os.path.exists(output_path):\n",
        "            os.remove(output_path)\n",
        "            \n",
        "        # Command line ffmpeg\n",
        "        # -i input: file ƒë·∫ßu v√†o\n",
        "        # -ar 16000: Sample rate 16k\n",
        "        # -ac 1: Mono channel (Pyannote t·ªët nh·∫•t v·ªõi mono)\n",
        "        # -y: Overwrite output\n",
        "        command = [\n",
        "            \"ffmpeg\", \n",
        "            \"-i\", audio_path,\n",
        "            \"-ar\", \"16000\",\n",
        "            \"-ac\", \"1\",\n",
        "            \"-y\",\n",
        "            output_path\n",
        "        ]\n",
        "        \n",
        "        subprocess.run(command, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "        return output_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting audio: {e}\")\n",
        "        # Fallback: Tr·∫£ v·ªÅ file g·ªëc n·∫øu convert l·ªói (d√π r·ªßi ro)\n",
        "        return audio_path\n",
        "\n",
        "def load_whisper_model(model_name, comp_type):\n",
        "    \"\"\"Dynamic load Whisper model v·ªõi cache\"\"\"\n",
        "    global loaded_whisper_models\n",
        "    cache_key = f\"{model_name}_{comp_type}\"\n",
        "    \n",
        "    if cache_key in loaded_whisper_models:\n",
        "        return loaded_whisper_models[cache_key]\n",
        "    \n",
        "    model_path = AVAILABLE_MODELS[model_name]\n",
        "    print(f\"Loading {model_name}...\")\n",
        "    start = time.time()\n",
        "    \n",
        "    model = WhisperModel(\n",
        "        model_path,\n",
        "        device=device,\n",
        "        compute_type=comp_type\n",
        "    )\n",
        "    \n",
        "    loaded_whisper_models[cache_key] = model\n",
        "    print(f\"‚úÖ Loaded in {time.time() - start:.1f}s\")\n",
        "    return model\n",
        "\n",
        "def format_timestamp(seconds):\n",
        "    \"\"\"Format seconds to MM:SS.ms\"\"\"\n",
        "    hours = int(seconds // 3600)\n",
        "    minutes = int((seconds % 3600) // 60)\n",
        "    secs = seconds % 60\n",
        "    if hours > 0:\n",
        "        return f\"{hours:02d}:{minutes:02d}:{secs:05.2f}\"\n",
        "    return f\"{minutes:02d}:{secs:05.2f}\"\n",
        "\n",
        "def assign_speaker_to_segment(seg_start, seg_end, diarization_result):\n",
        "    \"\"\"G√°n speaker cho segment d·ª±a tr√™n t·ª∑ l·ªá overlap >= 30%.\"\"\"\n",
        "    if diarization_result is None:\n",
        "        return \"SPEAKER_00\"\n",
        "    \n",
        "    seg_duration = seg_end - seg_start\n",
        "    if seg_duration <= 0:\n",
        "        return \"SPEAKER_00\"\n",
        "    \n",
        "    speaker_overlaps = {}\n",
        "    \n",
        "    for turn, _, speaker in diarization_result.speaker_diarization.itertracks(yield_label=True):\n",
        "        overlap_start = max(seg_start, turn.start)\n",
        "        overlap_end = min(seg_end, turn.end)\n",
        "        overlap = max(0, overlap_end - overlap_start)\n",
        "        \n",
        "        if overlap > 0:\n",
        "            if speaker not in speaker_overlaps:\n",
        "                speaker_overlaps[speaker] = 0\n",
        "            speaker_overlaps[speaker] += overlap\n",
        "    \n",
        "    if not speaker_overlaps:\n",
        "        return \"SPEAKER_00\"\n",
        "    \n",
        "    best_speaker = max(speaker_overlaps, key=speaker_overlaps.get)\n",
        "    best_overlap = speaker_overlaps[best_speaker]\n",
        "    \n",
        "    if best_overlap / seg_duration >= 0.3:\n",
        "        return best_speaker\n",
        "    \n",
        "    return \"SPEAKER_00\"\n",
        "\n",
        "def merge_consecutive_segments(segments, max_gap=0.5):\n",
        "    \"\"\"G·ªôp c√°c segment li√™n ti·∫øp c·ªßa c√πng m·ªôt speaker.\"\"\"\n",
        "    if not segments:\n",
        "        return []\n",
        "    \n",
        "    merged = []\n",
        "    current = segments[0].copy()\n",
        "    \n",
        "    for seg in segments[1:]:\n",
        "        if seg['speaker'] == current['speaker'] and (seg['start'] - current['end']) <= max_gap:\n",
        "            current['end'] = seg['end']\n",
        "            current['text'] += ' ' + seg['text']\n",
        "        else:\n",
        "            merged.append(current)\n",
        "            current = seg.copy()\n",
        "    \n",
        "    merged.append(current)\n",
        "    return merged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 5. ‚öôÔ∏è Processing Logic\n",
        "def process_audio(audio_path, model_name, language, beam_size, vad_filter, vad_min_silence, vad_speech_pad, vad_min_speech, vad_threshold, temperature, best_of, patience, length_penalty, initial_prompt, prefix, condition_on_previous_text, no_speech_threshold, log_prob_threshold, compression_ratio_threshold, comp_type, merge_segs, p=gr.Progress()):\n",
        "    \"\"\"\n",
        "    Quy tr√¨nh m·ªõi:\n",
        "    0. Chu·∫©n h√≥a audio (convert mp3 -> wav 16k).\n",
        "    1. Diarization ƒë·ªÉ t√°ch c√°c ƒëo·∫°n c·ªßa t·ª´ng ng∆∞·ªùi n√≥i.\n",
        "    2. C·∫Øt audio theo c√°c ƒëo·∫°n n√†y.\n",
        "    3. Transcribe t·ª´ng ƒëo·∫°n audio.\n",
        "    4. G·ªôp k·∫øt qu·∫£.\n",
        "    \"\"\"\n",
        "    if audio_path is None:\n",
        "        msg = \"‚ö†Ô∏è Vui l√≤ng upload ho·∫∑c ghi √¢m audio!\"\n",
        "        return msg, msg\n",
        "    \n",
        "    total_start_time = time.time()\n",
        "    \n",
        "    # Check Pyannote\n",
        "    if diarization_pipeline is None:\n",
        "        return \"‚ùå L·ªói: Ch∆∞a load ƒë∆∞·ª£c Pyannote (ki·ªÉm tra HF_TOKEN).\", \"‚ùå L·ªói: Ch∆∞a load ƒë∆∞·ª£c Pyannote.\"\n",
        "\n",
        "    # 0. Preprocessing Audio (Standardize)\n",
        "    p(0.05, desc=\"ƒêang chu·∫©n h√≥a audio (16kHz WAV)...\")\n",
        "    try:\n",
        "        # Lu√¥n convert v·ªÅ wav 16k mono ƒë·ªÉ tr√°nh l·ªói sample rate mismatch c·ªßa Pyannote\n",
        "        clean_audio_path = convert_audio_to_wav(audio_path)\n",
        "    except Exception as e:\n",
        "        msg = f\"‚ùå L·ªói convert audio: {e}\"\n",
        "        return msg, msg\n",
        "        \n",
        "    # 1. Load Standardized Audio for slicing later\n",
        "    p(0.08, desc=\"ƒêang ƒë·ªçc file audio...\")\n",
        "    try:\n",
        "        y, sr = librosa.load(clean_audio_path, sr=16000)\n",
        "        # sr should be 16000 now exactly\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå L·ªói ƒë·ªçc audio: {e}\", f\"‚ùå L·ªói ƒë·ªçc audio: {e}\"\n",
        "\n",
        "    # 2. DIARIZATION\n",
        "    p(0.1, desc=\"ƒêang ph√¢n t√°ch ng∆∞·ªùi n√≥i (Diarization)...\")\n",
        "    \n",
        "    try:\n",
        "        # S·ª≠ d·ª•ng file ƒë√£ chu·∫©n h√≥a\n",
        "        diarization = diarization_pipeline(clean_audio_path)\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå L·ªói Diarization: {e}\", f\"‚ùå L·ªói Diarization: {e}\"\n",
        "        \n",
        "    diarization_segments = []\n",
        "    # D√πng c√°ch user ƒë√£ fix tr∆∞·ªõc ƒë√≥ (n·∫øu model tr·∫£ v·ªÅ object kh√°c)\n",
        "    # M·∫∑c ƒë·ªãnh pipeline community tr·∫£ v·ªÅ Annotation tr·ª±c ti·∫øp, nh∆∞ng user fix th√†nh diarization.speaker_diarization\n",
        "    # M√¨nh s·∫Ω try/except ƒë·ªÉ support c·∫£ 2 structure cho an to√†n\n",
        "    try:\n",
        "        # Tr∆∞·ªùng h·ª£p 1: Standard Annotation\n",
        "        iterator = diarization.itertracks(yield_label=True)\n",
        "        # Test th·ª≠ xem c√≥ ch·∫°y ko, n·∫øu kh√¥ng ph·∫£i Annotation n√≥ s·∫Ω l·ªói attribute\n",
        "        _ = list(iterator)\n",
        "        # Reset iterate\n",
        "        iterator = diarization.itertracks(yield_label=True)\n",
        "    except:\n",
        "        # Tr∆∞·ªùng h·ª£p 2: User report structure (maybe wrapper)\n",
        "        try:\n",
        "             iterator = diarization.speaker_diarization.itertracks(yield_label=True)\n",
        "        except:\n",
        "             return \"‚ùå L·ªói format result Diarization\", \"‚ùå L·ªói format result Diarization\"\n",
        "\n",
        "    for turn, _, speaker in iterator:\n",
        "        diarization_segments.append({\n",
        "            \"start\": turn.start,\n",
        "            \"end\": turn.end,\n",
        "            \"speaker\": speaker\n",
        "        })\n",
        "    \n",
        "    # Sort segments by start time\n",
        "    diarization_segments.sort(key=lambda x: x['start'])\n",
        "    \n",
        "    # Merge consecutive segments if requested\n",
        "    if merge_segs and diarization_segments:\n",
        "        p(0.3, desc=\"ƒêang g·ªôp segment li√™n ti·∫øp...\")\n",
        "        merged = []\n",
        "        current = diarization_segments[0].copy()\n",
        "        for seg in diarization_segments[1:]:\n",
        "            if seg['speaker'] == current['speaker'] and (seg['start'] - current['end']) <= 0.5:\n",
        "                current['end'] = seg['end']\n",
        "            else:\n",
        "                merged.append(current)\n",
        "                current = seg.copy()\n",
        "        merged.append(current)\n",
        "        diarization_segments = merged\n",
        "    \n",
        "    # 3. TRANSCRIPTION LOOP\n",
        "    p(0.4, desc=\"ƒêang t·∫£i model Whisper...\")\n",
        "    model = load_whisper_model(model_name, comp_type)\n",
        "    \n",
        "    processed_segments = []\n",
        "    \n",
        "    total_segs = len(diarization_segments)\n",
        "    \n",
        "    # Prepare VAD options\n",
        "    if vad_filter:\n",
        "        vad_options = dict(\n",
        "            min_silence_duration_ms=vad_min_silence,\n",
        "            speech_pad_ms=vad_speech_pad,\n",
        "            min_speech_duration_ms=vad_min_speech,\n",
        "            threshold=vad_threshold\n",
        "        )\n",
        "    else:\n",
        "        vad_options = False\n",
        "        \n",
        "    prompt = initial_prompt.strip() if (initial_prompt and initial_prompt.strip()) else None\n",
        "    prefix_text = prefix.strip() if (prefix and prefix.strip()) else None\n",
        "\n",
        "    print(f\"Processing {total_segs} segments...\")\n",
        "    \n",
        "    for idx, seg in enumerate(diarization_segments):\n",
        "        start_sec = seg['start']\n",
        "        end_sec = seg['end']\n",
        "        speaker = seg['speaker']\n",
        "        \n",
        "        # UI Progress\n",
        "        progress_val = 0.4 + (0.5 * (idx / total_segs))\n",
        "        p(progress_val, desc=f\"Transcribing {idx+1}/{total_segs} ({speaker})...\")\n",
        "        \n",
        "        # Audio slicing\n",
        "        start_sample = int(start_sec * sr)\n",
        "        end_sample = int(end_sec * sr)\n",
        "        \n",
        "        # Avoid empty slice\n",
        "        if end_sample <= start_sample:\n",
        "            continue\n",
        "            \n",
        "        y_seg = y[start_sample:end_sample]\n",
        "        \n",
        "        # Whisper Transcribe for this chunk\n",
        "        try:\n",
        "            # Note: We pass the numpy array 'y_seg' directly\n",
        "            segments_gen, _ = model.transcribe(\n",
        "                y_seg, \n",
        "                language=language if language != \"auto\" else None,\n",
        "                beam_size=beam_size, \n",
        "                vad_filter=vad_options,\n",
        "                temperature=temperature,\n",
        "                best_of=best_of,\n",
        "                patience=patience,\n",
        "                length_penalty=length_penalty,\n",
        "                initial_prompt=prompt,\n",
        "                prefix=prefix_text,\n",
        "                condition_on_previous_text=condition_on_previous_text,\n",
        "                no_speech_threshold=no_speech_threshold,\n",
        "                log_prob_threshold=log_prob_threshold,\n",
        "                compression_ratio_threshold=compression_ratio_threshold,\n",
        "                word_timestamps=False \n",
        "            )\n",
        "            \n",
        "            # Collect text\n",
        "            seg_text_parts = []\n",
        "            for s in segments_gen:\n",
        "                seg_text_parts.append(s.text.strip())\n",
        "            \n",
        "            final_text = \" \".join(seg_text_parts).strip()\n",
        "            \n",
        "            if final_text:\n",
        "                # Store Result\n",
        "                processed_segments.append({\n",
        "                    \"start\": start_sec,\n",
        "                    \"end\": end_sec,\n",
        "                    \"speaker\": speaker,\n",
        "                    \"text\": final_text\n",
        "                })\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"Error transcribing segment {idx}: {e}\")\n",
        "            continue\n",
        "\n",
        "    total_elapsed = time.time() - total_start_time\n",
        "    \n",
        "    p(0.95, desc=\"ƒêang xu·∫•t k·∫øt qu·∫£...\")\n",
        "    \n",
        "    # ========== OUTPUT GENERATION ==========\n",
        "    \n",
        "    # Speaker colors\n",
        "    speaker_colors = {\n",
        "        'SPEAKER_00': 'üîµ',\n",
        "        'SPEAKER_01': 'üü¢', \n",
        "        'SPEAKER_02': 'üü°',\n",
        "        'SPEAKER_03': 'üü†',\n",
        "        'SPEAKER_04': 'üî¥',\n",
        "        'SPEAKER_05': 'üü£',\n",
        "    }\n",
        "    \n",
        "    # 1. Plain Transcription Output\n",
        "    transcribe_lines = []\n",
        "    for item in processed_segments:\n",
        "        ts = f\"[{format_timestamp(item['start'])} ‚Üí {format_timestamp(item['end'])}]\"\n",
        "        transcribe_lines.append(f\"{ts} {item['text']}\")\n",
        "        \n",
        "    transcribe_header = f\"\"\"## üìù K·∫øt qu·∫£ Transcription\n",
        "\n",
        "| Th√¥ng tin | Gi√° tr·ªã |\n",
        "|-----------|----------|\n",
        "| ‚è±Ô∏è T·ªïng th·ªùi gian x·ª≠ l√Ω | {total_elapsed:.1f}s |\n",
        "| üìä T·ªïng s·ªë Segment | {len(processed_segments)} |\n",
        "\n",
        "---\n",
        "\n",
        "\"\"\"\n",
        "    transcribe_output = transcribe_header + \"\\n\".join(transcribe_lines)\n",
        "    \n",
        "    # 2. Diarization + Transcription Output\n",
        "    diarize_lines = []\n",
        "    unique_speakers = set()\n",
        "    \n",
        "    for item in processed_segments:\n",
        "        unique_speakers.add(item['speaker'])\n",
        "        ts = f\"[{format_timestamp(item['start'])} ‚Üí {format_timestamp(item['end'])}]\"\n",
        "        icon = speaker_colors.get(item['speaker'], '‚ö™')\n",
        "        diarize_lines.append(f\"{ts} {icon} **{item['speaker']}**: {item['text']}\")\n",
        "        \n",
        "    diarize_header = f\"\"\"## üé≠ K·∫øt qu·∫£ Transcription + Diarization\n",
        "\n",
        "| Th√¥ng tin | Gi√° tr·ªã |\n",
        "|-----------|----------|\n",
        "| üë• S·ªë ng∆∞·ªùi n√≥i | {len(unique_speakers)} |\n",
        "| ‚è±Ô∏è T·ªïng th·ªùi gian x·ª≠ l√Ω | {total_elapsed:.1f}s |\n",
        "| üìä T·ªïng s·ªë Segment | {len(processed_segments)} |\n",
        "\n",
        "---\n",
        "\n",
        "\"\"\"\n",
        "    diarize_output = diarize_header + \"\\n\".join(diarize_lines)\n",
        "    \n",
        "    return transcribe_output, diarize_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 6. üöÄ Gradio UI\n",
        "css = \"\"\"\n",
        ".gradio-container { max-width: 1200px !important; }\n",
        ".output-markdown { font-family: 'JetBrains Mono', monospace !important; }\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(title=\"PrecisionVoice\", theme=gr.themes.Soft(), css=css) as demo:\n",
        "    gr.Markdown(\"\"\"# üéôÔ∏è PrecisionVoice - Vietnamese Speech-to-Text\n",
        "    \n",
        "S·ª≠ d·ª•ng **Whisper** ƒë·ªÉ nh·∫≠n d·∫°ng vƒÉn b·∫£n v√† **Pyannote** ƒë·ªÉ ph√¢n bi·ªát ng∆∞·ªùi n√≥i.\n",
        "\"\"\")\n",
        "    \n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            audio_input = gr.Audio(\n",
        "                sources=[\"upload\", \"microphone\"], \n",
        "                type=\"filepath\", \n",
        "                label=\"üîä Audio Input\"\n",
        "            )\n",
        "            \n",
        "            gr.Markdown(\"### ‚öôÔ∏è C√†i ƒë·∫∑t Model\")\n",
        "            model_select = gr.Dropdown(\n",
        "                choices=list(AVAILABLE_MODELS.keys()),\n",
        "                value=list(AVAILABLE_MODELS.keys())[0],\n",
        "                label=\"ü§ñ Whisper Model\"\n",
        "            )\n",
        "            \n",
        "            language = gr.Dropdown(\n",
        "                choices=[\"auto\", \"vi\", \"en\", \"zh\", \"ja\", \"ko\"],\n",
        "                value=\"vi\",\n",
        "                label=\"üåê Ng√¥n ng·ªØ\"\n",
        "            )\n",
        "            \n",
        "            comp_type_select = gr.Dropdown(\n",
        "                choices=[\"float16\", \"float32\", \"int8\", \"int8_float16\"],\n",
        "                value=compute_type,\n",
        "                label=\"‚ö° Compute Type\"\n",
        "            )\n",
        "            \n",
        "            with gr.Accordion(\"üîß T√πy ch·ªçn n√¢ng cao\", open=False):\n",
        "                beam_size = gr.Slider(\n",
        "                    minimum=1, maximum=10, value=5, step=1,\n",
        "                    label=\"Beam Size\",\n",
        "                    info=\"Cao h∆°n = ch√≠nh x√°c h∆°n nh∆∞ng ch·∫≠m h∆°n\"\n",
        "                )\n",
        "                vad_filter = gr.Checkbox(\n",
        "                    value=True, \n",
        "                    label=\"VAD Filter\",\n",
        "                    info=\"L·ªçc kho·∫£ng l·∫∑ng t·ª± ƒë·ªông\"\n",
        "                )\n",
        "                with gr.Row():\n",
        "                    vad_min_silence = gr.Number(value=1000, label=\"Min Silence (ms)\", info=\"min_silence_duration_ms\")\n",
        "                    vad_speech_pad = gr.Number(value=400, label=\"Speech Pad (ms)\", info=\"speech_pad_ms\")\n",
        "                with gr.Row():\n",
        "                    vad_min_speech = gr.Number(value=250, label=\"Min Speech (ms)\", info=\"min_speech_duration_ms\")\n",
        "                    vad_threshold = gr.Slider(minimum=0, maximum=1, value=0.5, step=0.05, label=\"VAD Threshold\")\n",
        "            \n",
        "            with gr.Accordion(\"üß† Tham s·ªë Generation (Whisper)\", open=False):\n",
        "                with gr.Row():\n",
        "                    temperature = gr.Slider(0.0, 1.0, value=0.0, step=0.1, label=\"Temperature\")\n",
        "                    best_of = gr.Number(value=5, label=\"Best Of\")\n",
        "                with gr.Row():\n",
        "                    patience = gr.Number(value=1.0, label=\"Patience\", step=0.1)\n",
        "                    length_penalty = gr.Number(value=1.0, label=\"Length Penalty\", step=0.1)\n",
        "                initial_prompt = gr.Textbox(label=\"Initial Prompt\", placeholder=\"Ng·ªØ c·∫£nh ho·∫∑c t·ª´ v·ª±ng...\")\n",
        "                prefix = gr.Textbox(label=\"Prefix\", placeholder=\"B·∫Øt ƒë·∫ßu c√¢u v·ªõi...\")\n",
        "                condition_on_previous_text = gr.Checkbox(value=True, label=\"Condition on previous text\")\n",
        "                \n",
        "                gr.Markdown(\"**Filter Thresholds**\")\n",
        "                with gr.Row():\n",
        "                    no_speech_threshold = gr.Slider(0.0, 1.0, value=0.6, step=0.05, label=\"No Speech Threshold\")\n",
        "                    log_prob_threshold = gr.Slider(-5.0, 0.0, value=-1.0, step=0.1, label=\"Log Prob Threshold\")\n",
        "                    compression_ratio_threshold = gr.Number(value=2.4, label=\"Compression Ratio Threshold\")\n",
        "            \n",
        "            merge_segments = gr.Checkbox(\n",
        "                value=True,\n",
        "                label=\"G·ªôp Segment c√πng Speaker\",\n",
        "                info=\"G·ªôp c√°c c√¢u li√™n ti·∫øp c·ªßa c√πng ng∆∞·ªùi n√≥i\"\n",
        "            )\n",
        "            \n",
        "            btn_process = gr.Button(\"üöÄ X·ª≠ l√Ω Audio\", variant=\"primary\", size=\"lg\")\n",
        "        \n",
        "        with gr.Column(scale=2):\n",
        "            with gr.Tabs():\n",
        "                with gr.Tab(\"üìù Transcription\"):\n",
        "                    output_transcribe = gr.Markdown(\n",
        "                        value=\"*K·∫øt qu·∫£ transcription s·∫Ω hi·ªÉn th·ªã ·ªü ƒë√¢y...*\",\n",
        "                        elem_classes=[\"output-markdown\"]\n",
        "                    )\n",
        "                with gr.Tab(\"üé≠ Transcription + Diarization\"):\n",
        "                    output_diarize = gr.Markdown(\n",
        "                        value=\"*K·∫øt qu·∫£ transcription + diarization s·∫Ω hi·ªÉn th·ªã ·ªü ƒë√¢y...*\",\n",
        "                        elem_classes=[\"output-markdown\"]\n",
        "                    )\n",
        "    \n",
        "    btn_process.click(\n",
        "        process_audio,\n",
        "        inputs=[\n",
        "            audio_input, model_select, language, beam_size, vad_filter, \n",
        "            vad_min_silence, vad_speech_pad, vad_min_speech, vad_threshold,\n",
        "            temperature, best_of, patience, length_penalty, \n",
        "            initial_prompt, prefix, condition_on_previous_text,\n",
        "            no_speech_threshold, log_prob_threshold, compression_ratio_threshold,\n",
        "            comp_type_select, merge_segments\n",
        "        ],\n",
        "        outputs=[output_transcribe, output_diarize]\n",
        "    )\n",
        "    \n",
        "    gr.Markdown(\"\"\"---\n",
        "    \n",
        "### üìñ H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng\n",
        "\n",
        "1. **Upload audio** ho·∫∑c ghi √¢m tr·ª±c ti·∫øp\n",
        "2. **Ch·ªçn Model**:\n",
        "   - `EraX-WoW-Turbo`: Whisper Large V3 Turbo, t·ªëi ∆∞u cho ti·∫øng Vi·ªát\n",
        "   - `PhoWhisper Large`: Model ƒë∆∞·ª£c hu·∫•n luy·ªán ri√™ng cho ti·∫øng Vi·ªát\n",
        "3. **Setting n√¢ng cao**:\n",
        "   - Ch·ªânh `temperature` n·∫øu mu·ªën model s√°ng t·∫°o h∆°n.\n",
        "   - Th√™m `Initial Prompt` ƒë·ªÉ g·ª£i √Ω t·ª´ v·ª±ng chuy√™n ng√†nh.\n",
        "4. **Nh·∫•n \"üöÄ X·ª≠ l√Ω Audio\"** ƒë·ªÉ nh·∫≠n k·∫øt qu·∫£ ·ªü c·∫£ 2 tab\n",
        "\"\"\")\n",
        "\n",
        "# Launch\n",
        "import os\n",
        "if \"COLAB_GPU\" in os.environ or \"google.colab\" in str(get_ipython()):\n",
        "    demo.queue().launch(share=True, debug=True)\n",
        "else:\n",
        "    demo.launch(share=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}